{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unveiling Insights into the Hong Kong YouTube Scene: A Data-Driven Exploration of Top Channels and User Sentiment\n",
    "\n",
    "## by Patty Lau\n",
    "\n",
    "### Description: This is an exploratory data analysis project focused on the most-subscribed YouTube video channels in Hong Kong. A range of tools and techniques were utilised to build a data pipeline and perform sentiment analysis on video comments. Data visualizations were also created to enhance insights and communicate findings.\n",
    "\n",
    "### Tech Stack: Python, Jupyter Notebook, MongoDB, Google Cloud Platform Compute Engine, Big Query, Natural Language API, Apache Spark, Microsoft Power BI, Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import datetime as dt\n",
    "import isodate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Connect to locatlhost and port 27017\n",
    "client = MongoClient(os.getenv('MONGODB_URI') or 'mongodb://localhost:27017')\n",
    "\n",
    "# Choose the MongoDB database\n",
    "db = client.youtubedataapi0416\n",
    "\n",
    "# Connect to the posgresql data warehouse\n",
    "POSTGRES_DB = os.getenv('POSTGRES_DB')\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect data from YouTube API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Sample Python code for youtube.channels.list\n",
    "# See instructions for running these code samples locally:\n",
    "# https://developers.google.com/explorer-help/code-samples#python\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "api_key = os.getenv('YOUTUBE_API_KEY')\n",
    "\n",
    "# Get credentials and create an API client\n",
    "youtube = build(\n",
    "    api_service_name, api_version, developerKey=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee林芊妤 (ID: UCxCZqbizSsnntlz6w0fN8hA)\n",
      "Fanny Wang - Topic (ID: UCTEy93F0GaoP_ycEIgqWjbg)\n",
      "Stephy Tang - Topic (ID: UCtiL38iHIIUi10k_zbpqCRQ)\n"
     ]
    }
   ],
   "source": [
    "# search for channels\n",
    "\n",
    "# Define the search parameters\n",
    "search_query = [\"Emi Wong\", \"Coffee Lam\"]\n",
    "request = youtube.search().list(\n",
    "    q=\",\".join(search_query),\n",
    "    type=\"channel\",\n",
    "    part=\"id,snippet\",\n",
    "    maxResults=10\n",
    ")\n",
    "\n",
    "# Execute the search and print the results\n",
    "next_page_token = ''\n",
    "while True:\n",
    "    response = request.execute()\n",
    "    for channel in response['items']:\n",
    "        channel_id = channel['id']['channelId']\n",
    "        channel_name = channel['snippet']['title']\n",
    "        print(f\"{channel_name} (ID: {channel_id})\")\n",
    "\n",
    "    # Check if there are more pages of results\n",
    "    if 'nextPageToken' in response:\n",
    "        next_page_token = response['nextPageToken']\n",
    "        request = youtube.search().list(\n",
    "            q=\",\".join(search_query),\n",
    "            type=\"channel\",\n",
    "            part=\"id,snippet\",\n",
    "            maxResults=10,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to get channel stats\n",
    "\n",
    "def get_channel_stats(youtube, channel_ids):\n",
    "    all_channel_stats = []  # list to store all channel stats\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=','.join(channel_ids)\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # loop through each channel id\n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelId=response['items'][i]['id'],\n",
    "                    ChannelTitle=response['items'][i]['snippet']['title'],\n",
    "                    subscribers=int(\n",
    "                        response['items'][i]['statistics']['subscriberCount']),\n",
    "                    totalViews=int(response['items'][i]\n",
    "                                   ['statistics']['viewCount']),\n",
    "                    totalVideos=int(response['items']\n",
    "                                    [i]['statistics']['videoCount']),\n",
    "                    channelPublishedAt=response['items'][i]['snippet']['publishedAt'],\n",
    "                    playlistId=response['items'][i]['contentDetails']['relatedPlaylists']['uploads'],\n",
    "                    DataRetrievedAt=dt.datetime.now().isoformat())\n",
    "        all_channel_stats.append(data)\n",
    "\n",
    "        # Write data to MongoDB\n",
    "        # db.top5hkchannelsinfo.insert_one(data)\n",
    "    return all_channel_stats  # return all channel stats for each channel ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get videoIds from playlistId\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50)\n",
    "    response = request.execute()\n",
    "\n",
    "    video_ids = []\n",
    "\n",
    "    for i in range(len(response['items'])):\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    havePages = True\n",
    "\n",
    "    while havePages:\n",
    "        if next_page_token is None:\n",
    "            havePages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token)\n",
    "            response = request.execute()\n",
    "\n",
    "            for i in range(len(response['items'])):\n",
    "                video_ids.append(response['items'][i]\n",
    "                                 ['contentDetails']['videoId'])\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            # Write data to MongoDB\n",
    "    # db.allvideoids.insert_one(dict(playlistId=playlist_id,videoId=video_ids))\n",
    "\n",
    "    return video_ids  # return list of videoIds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get video details\n",
    "def get_video_details(youtube, video_ids):\n",
    "\n",
    "    all_video_info = []\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for video in response['items']:\n",
    "            if 'tags' not in video['snippet'].keys():\n",
    "                video['snippet']['tags'] = None\n",
    "            if 'likeCount' not in video['statistics'].keys():\n",
    "                video['statistics']['likeCount'] = 0\n",
    "            if 'commentCount' not in video['statistics'].keys():\n",
    "                video['statistics']['commentCount'] = 0\n",
    "            # insert video id\n",
    "            video_info = dict(VideoId=video['id'],\n",
    "                              ChannelTitle=video['snippet']['channelTitle'],\n",
    "                              VideoTitle=video['snippet']['title'],\n",
    "                              Description=video['snippet']['description'],\n",
    "                              Tags=video['snippet']['tags'],\n",
    "                              Published_date=video['snippet']['publishedAt'],\n",
    "                              Views=int(video['statistics']['viewCount']),\n",
    "                              Likes=int(video['statistics']['likeCount']),\n",
    "                              Comments=int(\n",
    "                                  video['statistics']['commentCount']),\n",
    "                              Duration=isodate.parse_duration(\n",
    "                                  video['contentDetails']['duration']).total_seconds(),\n",
    "                              Definition=video['contentDetails']['definition'],\n",
    "                              Caption=video['contentDetails']['caption'],\n",
    "                              DataRetrievedAt=dt.datetime.now().isoformat()\n",
    "                              )\n",
    "            all_video_info.append(video_info)\n",
    "            # db.allvideoinfo.insert_one(video_info)\n",
    "    return all_video_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get video comments\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "\n",
    "    all_comments_info = []\n",
    "    all_disabled_comments_info = []\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100  # set max number of comments to retrieve\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            comments_info = dict(videoId=video_id,\n",
    "                                 commentText=[comment['snippet']['topLevelComment']\n",
    "                                              ['snippet']['textOriginal'] for comment in response['items']],\n",
    "                                 commentPublishedAt=[comment['snippet']['topLevelComment']['snippet']\n",
    "                                                     ['publishedAt'] for comment in response['items']],\n",
    "                                 likeCount=[int(comment['snippet']['topLevelComment']['snippet']\n",
    "                                                    ['likeCount']) for comment in response['items']],\n",
    "                                 DataRetrievedAt=dt.datetime.now().isoformat()\n",
    "                                 )\n",
    "            all_comments_info.append(comments_info)\n",
    "            # Write data to MongoDB\n",
    "            # db.allcommentsinfo.insert_one(comments_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print(e)\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "\n",
    "            disabled_comments_info = dict(videoId=video_id, commentText=\"disabled comments\",\n",
    "                                          commentPublishedAt=\"disabled comments\",\n",
    "                                          likeCount=\"disabled comments\",\n",
    "                                          DataRetrievedAt=dt.datetime.now().isoformat()\n",
    "                                          )\n",
    "            all_disabled_comments_info.append(disabled_comments_info)\n",
    "            # db.disabledcommentsinfo.insert_one(disabled_comments_info)\n",
    "    return all_comments_info, all_disabled_comments_info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert raw data collected to mongoDB and display temperorary dataframe with Pandas\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get channel statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = ['UCvGEK5_U-kLgO6-AMDPeTUQ',  # Emi Wong\n",
    "               'UCxCZqbizSsnntlz6w0fN8hA',  # Coffee Lam\n",
    "               'UCXnWjmQ8BDE0sDIeZLK5yJg',  # 點 Cook Guide\n",
    "               'UC4nsi0oM9WBNFv1RdLh3c2g',  # JASON816\n",
    "               'UCDpK1rg5I9Zc3ToY13vbR3w'  # 笑波子\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data = get_channel_stats(youtube, channel_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelId</th>\n",
       "      <th>ChannelTitle</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>totalViews</th>\n",
       "      <th>totalVideos</th>\n",
       "      <th>channelPublishedAt</th>\n",
       "      <th>playlistId</th>\n",
       "      <th>DataRetrievedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCDpK1rg5I9Zc3ToY13vbR3w</td>\n",
       "      <td>笑波子</td>\n",
       "      <td>936000</td>\n",
       "      <td>1051897493</td>\n",
       "      <td>4354</td>\n",
       "      <td>2006-09-09T19:59:59Z</td>\n",
       "      <td>UUDpK1rg5I9Zc3ToY13vbR3w</td>\n",
       "      <td>2023-04-16T09:38:02.175488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC4nsi0oM9WBNFv1RdLh3c2g</td>\n",
       "      <td>JASON</td>\n",
       "      <td>1040000</td>\n",
       "      <td>521248369</td>\n",
       "      <td>2955</td>\n",
       "      <td>2013-06-16T13:50:59Z</td>\n",
       "      <td>UU4nsi0oM9WBNFv1RdLh3c2g</td>\n",
       "      <td>2023-04-16T09:38:02.175504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCXnWjmQ8BDE0sDIeZLK5yJg</td>\n",
       "      <td>點 Cook Guide</td>\n",
       "      <td>1110000</td>\n",
       "      <td>202603292</td>\n",
       "      <td>1356</td>\n",
       "      <td>2014-02-07T15:44:04Z</td>\n",
       "      <td>UUXnWjmQ8BDE0sDIeZLK5yJg</td>\n",
       "      <td>2023-04-16T09:38:02.175526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCvGEK5_U-kLgO6-AMDPeTUQ</td>\n",
       "      <td>emi wong</td>\n",
       "      <td>5960000</td>\n",
       "      <td>787128269</td>\n",
       "      <td>442</td>\n",
       "      <td>2014-11-02T14:43:34Z</td>\n",
       "      <td>UUvGEK5_U-kLgO6-AMDPeTUQ</td>\n",
       "      <td>2023-04-16T09:38:02.175529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCxCZqbizSsnntlz6w0fN8hA</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>1700000</td>\n",
       "      <td>261803319</td>\n",
       "      <td>353</td>\n",
       "      <td>2015-06-02T07:09:15Z</td>\n",
       "      <td>UUxCZqbizSsnntlz6w0fN8hA</td>\n",
       "      <td>2023-04-16T09:38:02.175533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channelId  ChannelTitle  subscribers  totalViews  \\\n",
       "0  UCDpK1rg5I9Zc3ToY13vbR3w           笑波子       936000  1051897493   \n",
       "1  UC4nsi0oM9WBNFv1RdLh3c2g         JASON      1040000   521248369   \n",
       "2  UCXnWjmQ8BDE0sDIeZLK5yJg  點 Cook Guide      1110000   202603292   \n",
       "3  UCvGEK5_U-kLgO6-AMDPeTUQ      emi wong      5960000   787128269   \n",
       "4  UCxCZqbizSsnntlz6w0fN8hA     Coffee林芊妤      1700000   261803319   \n",
       "\n",
       "   totalVideos    channelPublishedAt                playlistId  \\\n",
       "0         4354  2006-09-09T19:59:59Z  UUDpK1rg5I9Zc3ToY13vbR3w   \n",
       "1         2955  2013-06-16T13:50:59Z  UU4nsi0oM9WBNFv1RdLh3c2g   \n",
       "2         1356  2014-02-07T15:44:04Z  UUXnWjmQ8BDE0sDIeZLK5yJg   \n",
       "3          442  2014-11-02T14:43:34Z  UUvGEK5_U-kLgO6-AMDPeTUQ   \n",
       "4          353  2015-06-02T07:09:15Z  UUxCZqbizSsnntlz6w0fN8hA   \n",
       "\n",
       "              DataRetrievedAt  \n",
       "0  2023-04-16T09:38:02.175488  \n",
       "1  2023-04-16T09:38:02.175504  \n",
       "2  2023-04-16T09:38:02.175526  \n",
       "3  2023-04-16T09:38:02.175529  \n",
       "4  2023-04-16T09:38:02.175533  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(channel_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f1488c3ac20>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.top5hkchannelsinfo.insert_many(channel_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get video ids from playlist id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_id = \"UUxCZqbizSsnntlz6w0fN8hA\"  # emi\n",
    "video_ids = get_video_ids(youtube, playlist_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x7f1424b1a260>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.allvideoids.insert_one(dict(playlistId=playlist_id, videoId=video_ids))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get video details from video ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoId</th>\n",
       "      <th>ChannelTitle</th>\n",
       "      <th>VideoTitle</th>\n",
       "      <th>Description</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Published_date</th>\n",
       "      <th>Views</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Caption</th>\n",
       "      <th>DataRetrievedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WxTxeyqRM4E</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>趁着兒童節同大家宣佈CoffeeSweat代言人🌟陳伯個fd～翟伯😎!! #coffeesw...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2023-04-04T09:18:00Z</td>\n",
       "      <td>10622</td>\n",
       "      <td>207</td>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:16.977984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ibZ6t_lTTIA</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>28分鐘 女性日常養生瑜伽｜由內到外保養｜全身保健拉筋♥️愛自己♥️｜孕婦都能做</td>\n",
       "      <td>On coffee lam\\nWrap around long sleeve- white\\...</td>\n",
       "      <td>[coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...</td>\n",
       "      <td>2023-04-01T07:50:53Z</td>\n",
       "      <td>55057</td>\n",
       "      <td>1937</td>\n",
       "      <td>123</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:16.978017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1AqSRo535vs</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>22分鐘 極有效全身局部肌肉訓練｜用body weight增肌｜14天肌力挑戰🔥</td>\n",
       "      <td>On Coffee Lam\\nChic Sports Bra（Mercury Grey）\\n...</td>\n",
       "      <td>[coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...</td>\n",
       "      <td>2023-03-20T04:51:15Z</td>\n",
       "      <td>95416</td>\n",
       "      <td>2072</td>\n",
       "      <td>156</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:16.978034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ZLyuY01Dqo</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>每天都要穿着腰封去為腰部塑形🔥</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-18T04:44:13Z</td>\n",
       "      <td>15493</td>\n",
       "      <td>206</td>\n",
       "      <td>7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:16.978050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pmOdWGRcNvc</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>CoffeeSweat 2023 Spring Collection♥️</td>\n",
       "      <td>www.CoffeeSweat.com</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-13T08:52:26Z</td>\n",
       "      <td>8326</td>\n",
       "      <td>231</td>\n",
       "      <td>5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:16.978065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>11tdiQZb-BM</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>COFFEE YOGA EP4. 升CUP瑜珈！幫你減肚腩！</td>\n",
       "      <td>一連兩集升級瑜珈篇，第一集先教大家減！肚！腩 ！先有兩個動作改善寒背問題，再加兩個動作同幫你...</td>\n",
       "      <td>[coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...</td>\n",
       "      <td>2015-07-26T03:08:00Z</td>\n",
       "      <td>146074</td>\n",
       "      <td>1330</td>\n",
       "      <td>33</td>\n",
       "      <td>200.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:18.180162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>SUy3-htFkys</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>COFFEE YOGA EP3. 瑜珈男女大不同</td>\n",
       "      <td>男仔女仔做瑜珈動作各有各困難，今次就同大家簡單示範下啦！\\n想學更多，記得SUBSCRIBE...</td>\n",
       "      <td>[coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...</td>\n",
       "      <td>2015-07-18T05:30:01Z</td>\n",
       "      <td>171174</td>\n",
       "      <td>594</td>\n",
       "      <td>35</td>\n",
       "      <td>148.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:18.180173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>uFRn2Jg1vbk</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>COFFEE YOGA EP2. 私影瑜珈 (𡃁模MODELS必學)</td>\n",
       "      <td>想擺靚pose影相，SHOWSHOW靚身材？\\n今次就教各位美女們兩招動作旁身啦\\n想學更多...</td>\n",
       "      <td>[私影, 𡃁模, Yoga (Sport), Model (Profession), pos...</td>\n",
       "      <td>2015-07-10T12:32:18Z</td>\n",
       "      <td>67662</td>\n",
       "      <td>383</td>\n",
       "      <td>22</td>\n",
       "      <td>140.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:18.260663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>SCUXhKmOulc</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>COFFEE YOGA SPECIAL 柔軟咖啡 X 剛硬男仔（男女瑜珈沙灘篇）</td>\n",
       "      <td>今次去到沙灘遇上硬崩崩嘅伍仔\\n於是邀請佢同我一齊做瑜珈\\n希望向大家示範男女都啱做嘅瑜珈動...</td>\n",
       "      <td>[COFFEE, Yoga (Sport), Hong Kong (Country), 瑜珈...</td>\n",
       "      <td>2015-07-05T03:00:01Z</td>\n",
       "      <td>50866</td>\n",
       "      <td>345</td>\n",
       "      <td>36</td>\n",
       "      <td>205.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:18.260689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>iET9uY2luDQ</td>\n",
       "      <td>Coffee林芊妤</td>\n",
       "      <td>COFFEE YOGA EP1. 教你瑜珈基本三式</td>\n",
       "      <td>瑜珈基本三式，一教你就識！\\nCOFFEE 林芊妤 youtube channel 第一條片...</td>\n",
       "      <td>[COFFEE, YOGA, 林芊妤, 瑜珈]</td>\n",
       "      <td>2015-07-04T09:00:01Z</td>\n",
       "      <td>96398</td>\n",
       "      <td>1029</td>\n",
       "      <td>52</td>\n",
       "      <td>221.0</td>\n",
       "      <td>hd</td>\n",
       "      <td>false</td>\n",
       "      <td>2023-04-16T09:42:18.260702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VideoId ChannelTitle  \\\n",
       "0    WxTxeyqRM4E    Coffee林芊妤   \n",
       "1    ibZ6t_lTTIA    Coffee林芊妤   \n",
       "2    1AqSRo535vs    Coffee林芊妤   \n",
       "3    8ZLyuY01Dqo    Coffee林芊妤   \n",
       "4    pmOdWGRcNvc    Coffee林芊妤   \n",
       "..           ...          ...   \n",
       "348  11tdiQZb-BM    Coffee林芊妤   \n",
       "349  SUy3-htFkys    Coffee林芊妤   \n",
       "350  uFRn2Jg1vbk    Coffee林芊妤   \n",
       "351  SCUXhKmOulc    Coffee林芊妤   \n",
       "352  iET9uY2luDQ    Coffee林芊妤   \n",
       "\n",
       "                                            VideoTitle  \\\n",
       "0    趁着兒童節同大家宣佈CoffeeSweat代言人🌟陳伯個fd～翟伯😎!! #coffeesw...   \n",
       "1             28分鐘 女性日常養生瑜伽｜由內到外保養｜全身保健拉筋♥️愛自己♥️｜孕婦都能做   \n",
       "2             22分鐘 極有效全身局部肌肉訓練｜用body weight增肌｜14天肌力挑戰🔥   \n",
       "3                                      每天都要穿着腰封去為腰部塑形🔥   \n",
       "4                 CoffeeSweat 2023 Spring Collection♥️   \n",
       "..                                                 ...   \n",
       "348                     COFFEE YOGA EP4. 升CUP瑜珈！幫你減肚腩！   \n",
       "349                           COFFEE YOGA EP3. 瑜珈男女大不同   \n",
       "350                 COFFEE YOGA EP2. 私影瑜珈 (𡃁模MODELS必學)   \n",
       "351           COFFEE YOGA SPECIAL 柔軟咖啡 X 剛硬男仔（男女瑜珈沙灘篇）   \n",
       "352                          COFFEE YOGA EP1. 教你瑜珈基本三式   \n",
       "\n",
       "                                           Description  \\\n",
       "0                                                        \n",
       "1    On coffee lam\\nWrap around long sleeve- white\\...   \n",
       "2    On Coffee Lam\\nChic Sports Bra（Mercury Grey）\\n...   \n",
       "3                                                        \n",
       "4                                  www.CoffeeSweat.com   \n",
       "..                                                 ...   \n",
       "348  一連兩集升級瑜珈篇，第一集先教大家減！肚！腩 ！先有兩個動作改善寒背問題，再加兩個動作同幫你...   \n",
       "349  男仔女仔做瑜珈動作各有各困難，今次就同大家簡單示範下啦！\\n想學更多，記得SUBSCRIBE...   \n",
       "350  想擺靚pose影相，SHOWSHOW靚身材？\\n今次就教各位美女們兩招動作旁身啦\\n想學更多...   \n",
       "351  今次去到沙灘遇上硬崩崩嘅伍仔\\n於是邀請佢同我一齊做瑜珈\\n希望向大家示範男女都啱做嘅瑜珈動...   \n",
       "352  瑜珈基本三式，一教你就識！\\nCOFFEE 林芊妤 youtube channel 第一條片...   \n",
       "\n",
       "                                                  Tags        Published_date  \\\n",
       "0                                                 None  2023-04-04T09:18:00Z   \n",
       "1    [coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...  2023-04-01T07:50:53Z   \n",
       "2    [coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...  2023-03-20T04:51:15Z   \n",
       "3                                                 None  2023-03-18T04:44:13Z   \n",
       "4                                                 None  2023-03-13T08:52:26Z   \n",
       "..                                                 ...                   ...   \n",
       "348  [coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...  2015-07-26T03:08:00Z   \n",
       "349  [coffee, coffee lam, coffeelam, 林芊妤, coffeeyog...  2015-07-18T05:30:01Z   \n",
       "350  [私影, 𡃁模, Yoga (Sport), Model (Profession), pos...  2015-07-10T12:32:18Z   \n",
       "351  [COFFEE, Yoga (Sport), Hong Kong (Country), 瑜珈...  2015-07-05T03:00:01Z   \n",
       "352                            [COFFEE, YOGA, 林芊妤, 瑜珈]  2015-07-04T09:00:01Z   \n",
       "\n",
       "      Views  Likes  Comments  Duration Definition Caption  \\\n",
       "0     10622    207         5      13.0         hd   false   \n",
       "1     55057   1937       123    1762.0         hd   false   \n",
       "2     95416   2072       156    1470.0         hd   false   \n",
       "3     15493    206         7      17.0         hd   false   \n",
       "4      8326    231         5      61.0         hd   false   \n",
       "..      ...    ...       ...       ...        ...     ...   \n",
       "348  146074   1330        33     200.0         hd   false   \n",
       "349  171174    594        35     148.0         hd   false   \n",
       "350   67662    383        22     140.0         hd   false   \n",
       "351   50866    345        36     205.0         hd   false   \n",
       "352   96398   1029        52     221.0         hd   false   \n",
       "\n",
       "                DataRetrievedAt  \n",
       "0    2023-04-16T09:42:16.977984  \n",
       "1    2023-04-16T09:42:16.978017  \n",
       "2    2023-04-16T09:42:16.978034  \n",
       "3    2023-04-16T09:42:16.978050  \n",
       "4    2023-04-16T09:42:16.978065  \n",
       "..                          ...  \n",
       "348  2023-04-16T09:42:18.180162  \n",
       "349  2023-04-16T09:42:18.180173  \n",
       "350  2023-04-16T09:42:18.260663  \n",
       "351  2023-04-16T09:42:18.260689  \n",
       "352  2023-04-16T09:42:18.260702  \n",
       "\n",
       "[353 rows x 13 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_details_result = get_video_details(youtube, video_ids)\n",
    "pd.DataFrame(get_video_details_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f1425974eb0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.allvideodetails.insert_many(get_video_details_result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remove duplicate video details in the sentiment collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is to remove video details in the sentiment collection\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['youtubedataapi0412']\n",
    "collection = db['allcomments_sample_sentiment_0414']\n",
    "\n",
    "# Get a list of unique comments in the collection\n",
    "unique_comments = collection.distinct('Comment')\n",
    "\n",
    "# Iterate through the collection and remove any documents with duplicate comments\n",
    "for comment in unique_comments:\n",
    "    # Count the number of documents with this comment\n",
    "    count = collection.count_documents({'Comment': comment})\n",
    "\n",
    "    # If there is more than one document, delete all but one\n",
    "    if count > 1:\n",
    "        # Get all documents with this comment\n",
    "        documents = collection.find({'Comment': comment})\n",
    "\n",
    "        # Keep the first document and delete the rest\n",
    "        first_document = True\n",
    "        for document in documents:\n",
    "            if first_document:\n",
    "                first_document = False\n",
    "            else:\n",
    "                collection.delete_one({'_id': document['_id']})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get video comments from video ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet%2Creplies&videoId=NJZ95sZlXL4&maxResults=100&key=AIzaSyColQwCiZxVE9zGJEcZ54SjVCT0tcqY1xE&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\">\n",
      "Could not get comments for video NJZ95sZlXL4\n"
     ]
    }
   ],
   "source": [
    "all_comments_in_videos = get_comments_in_videos(youtube, video_ids)\n",
    "# pd.DataFrame(all_comments_in_videos[1])  # disabled comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoId</th>\n",
       "      <th>commentText</th>\n",
       "      <th>commentPublishedAt</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>DataRetrievedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pgGoBihIUiU</td>\n",
       "      <td>[Do you have exercise for foot sprain?, thanks...</td>\n",
       "      <td>[2023-04-15T09:53:26Z, 2023-04-15T09:37:06Z, 2...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2023-04-15T11:16:34.802343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G0cBlx-Jfdo</td>\n",
       "      <td>[Thank you emi. ❤, Recipe for the banana cake ...</td>\n",
       "      <td>[2023-04-14T00:01:09Z, 2023-04-12T01:12:11Z, 2...</td>\n",
       "      <td>[0, 1, 0, 5, 3, 3, 3, 7, 4, 3, 3, 4]</td>\n",
       "      <td>2023-04-15T11:16:35.048075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6sLXvyL-JEc</td>\n",
       "      <td>[this vlog was filmed more than a year ago😳 so...</td>\n",
       "      <td>[2023-04-09T12:52:58Z, 2023-04-14T05:19:00Z, 2...</td>\n",
       "      <td>[56, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>2023-04-15T11:16:35.342696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D8V_No7wxVQ</td>\n",
       "      <td>[Fun video! Thanks for sharing 👌, in this peri...</td>\n",
       "      <td>[2023-04-12T16:22:46Z, 2023-04-08T15:42:04Z, 2...</td>\n",
       "      <td>[0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 6, ...</td>\n",
       "      <td>2023-04-15T11:16:36.038890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MJapk3ZX5SE</td>\n",
       "      <td>[good job emi wong youre spanish sounds pretty...</td>\n",
       "      <td>[2023-04-08T15:47:21Z, 2023-04-06T09:34:54Z, 2...</td>\n",
       "      <td>[1, 1, 0, 1, 0, 6, 1, 0, 0, 2, 1, 6, 1, 0, 16,...</td>\n",
       "      <td>2023-04-15T11:16:36.294262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>WwqYFnBGjT8</td>\n",
       "      <td>[I also post what I eat everyday on my instagr...</td>\n",
       "      <td>[2019-07-16T10:24:46Z, 2022-07-02T04:45:57Z, 2...</td>\n",
       "      <td>[14, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 6,...</td>\n",
       "      <td>2023-04-15T11:19:37.889696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>DZ6voLaG9_I</td>\n",
       "      <td>[I also post what I eat everyday on my instagr...</td>\n",
       "      <td>[2019-07-16T08:54:18Z, 2023-03-09T13:32:52Z, 2...</td>\n",
       "      <td>[255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0...</td>\n",
       "      <td>2023-04-15T11:19:38.265562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Fo470c993yA</td>\n",
       "      <td>[I also post what I eat everyday on my instagr...</td>\n",
       "      <td>[2019-07-16T10:18:01Z, 2022-03-10T12:28:13Z, 2...</td>\n",
       "      <td>[18, 0, 0, 0, 3, 3, 0, 0, 5, 6, 1, 0, 1, 0, 5,...</td>\n",
       "      <td>2023-04-15T11:19:38.661458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>p1DJhMy0yCs</td>\n",
       "      <td>[I also post what I eat everyday on my instagr...</td>\n",
       "      <td>[2019-07-16T09:54:35Z, 2023-04-09T15:09:19Z, 2...</td>\n",
       "      <td>[87, 0, 1, 0, 0, 3, 0, 1, 1, 0, 0, 0, 3, 0, 1,...</td>\n",
       "      <td>2023-04-15T11:19:39.284293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>zuXFRfbTmiU</td>\n",
       "      <td>[I also post what I eat everyday on my instagr...</td>\n",
       "      <td>[2019-07-16T10:04:25Z, 2023-03-16T20:19:18Z, 2...</td>\n",
       "      <td>[251, 0, 0, 0, 2, 0, 6, 1, 0, 0, 0, 3, 2, 0, 0...</td>\n",
       "      <td>2023-04-15T11:19:39.753356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         videoId                                        commentText  \\\n",
       "0    pgGoBihIUiU  [Do you have exercise for foot sprain?, thanks...   \n",
       "1    G0cBlx-Jfdo  [Thank you emi. ❤, Recipe for the banana cake ...   \n",
       "2    6sLXvyL-JEc  [this vlog was filmed more than a year ago😳 so...   \n",
       "3    D8V_No7wxVQ  [Fun video! Thanks for sharing 👌, in this peri...   \n",
       "4    MJapk3ZX5SE  [good job emi wong youre spanish sounds pretty...   \n",
       "..           ...                                                ...   \n",
       "437  WwqYFnBGjT8  [I also post what I eat everyday on my instagr...   \n",
       "438  DZ6voLaG9_I  [I also post what I eat everyday on my instagr...   \n",
       "439  Fo470c993yA  [I also post what I eat everyday on my instagr...   \n",
       "440  p1DJhMy0yCs  [I also post what I eat everyday on my instagr...   \n",
       "441  zuXFRfbTmiU  [I also post what I eat everyday on my instagr...   \n",
       "\n",
       "                                    commentPublishedAt  \\\n",
       "0    [2023-04-15T09:53:26Z, 2023-04-15T09:37:06Z, 2...   \n",
       "1    [2023-04-14T00:01:09Z, 2023-04-12T01:12:11Z, 2...   \n",
       "2    [2023-04-09T12:52:58Z, 2023-04-14T05:19:00Z, 2...   \n",
       "3    [2023-04-12T16:22:46Z, 2023-04-08T15:42:04Z, 2...   \n",
       "4    [2023-04-08T15:47:21Z, 2023-04-06T09:34:54Z, 2...   \n",
       "..                                                 ...   \n",
       "437  [2019-07-16T10:24:46Z, 2022-07-02T04:45:57Z, 2...   \n",
       "438  [2019-07-16T08:54:18Z, 2023-03-09T13:32:52Z, 2...   \n",
       "439  [2019-07-16T10:18:01Z, 2022-03-10T12:28:13Z, 2...   \n",
       "440  [2019-07-16T09:54:35Z, 2023-04-09T15:09:19Z, 2...   \n",
       "441  [2019-07-16T10:04:25Z, 2023-03-16T20:19:18Z, 2...   \n",
       "\n",
       "                                             likeCount  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1                 [0, 1, 0, 5, 3, 3, 3, 7, 4, 3, 3, 4]   \n",
       "2    [56, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3    [0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 6, ...   \n",
       "4    [1, 1, 0, 1, 0, 6, 1, 0, 0, 2, 1, 6, 1, 0, 16,...   \n",
       "..                                                 ...   \n",
       "437  [14, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 6,...   \n",
       "438  [255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0...   \n",
       "439  [18, 0, 0, 0, 3, 3, 0, 0, 5, 6, 1, 0, 1, 0, 5,...   \n",
       "440  [87, 0, 1, 0, 0, 3, 0, 1, 1, 0, 0, 0, 3, 0, 1,...   \n",
       "441  [251, 0, 0, 0, 2, 0, 6, 1, 0, 0, 0, 3, 2, 0, 0...   \n",
       "\n",
       "                DataRetrievedAt  \n",
       "0    2023-04-15T11:16:34.802343  \n",
       "1    2023-04-15T11:16:35.048075  \n",
       "2    2023-04-15T11:16:35.342696  \n",
       "3    2023-04-15T11:16:36.038890  \n",
       "4    2023-04-15T11:16:36.294262  \n",
       "..                          ...  \n",
       "437  2023-04-15T11:19:37.889696  \n",
       "438  2023-04-15T11:19:38.265562  \n",
       "439  2023-04-15T11:19:38.661458  \n",
       "440  2023-04-15T11:19:39.284293  \n",
       "441  2023-04-15T11:19:39.753356  \n",
       "\n",
       "[442 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_comments_in_videos[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f8a24c816f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.allcommentsinfo2.insert_many(all_comments_in_videos[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unwind comments collected in mongoDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x7f8a24cc3f40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unwind the arrays inside all comments info document\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# access the database\n",
    "db = client['youtubedataapi0415']\n",
    "\n",
    "# access the collection\n",
    "collection = db['allcommentsinfo2']\n",
    "\n",
    "# aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        '$unwind': {\n",
    "            'path': '$commentText',\n",
    "            'includeArrayIndex': 'arrayIndex'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'videoId': 1,\n",
    "            'commentText': 1,\n",
    "            'commentPublishedAt': {'$arrayElemAt': ['$commentPublishedAt', '$arrayIndex']},\n",
    "            'likeCount': {'$arrayElemAt': ['$likeCount', '$arrayIndex']},\n",
    "            'DataRetrievedAt': 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$out': 'allcommentsinfo_unwinded2'\n",
    "    }\n",
    "]\n",
    "\n",
    "# execute aggregation pipeline\n",
    "collection.aggregate(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating small size test samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectId('64366db41663f97dab7cf6a2'), ObjectId('64366db41663f97dab7cf6a3'), ObjectId('64366db41663f97dab7cf6a4'), ObjectId('64366db41663f97dab7cf6a5'), ObjectId('64366db41663f97dab7cf6a6')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pymongo.command_cursor.CommandCursor at 0x7f00d1b7bd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# get the first 5 documents in allcommentsinfo for testing\n",
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "db = client['youtubedataapi0412']\n",
    "\n",
    "# Select the first 10 documents in the collection\n",
    "pipeline = [\n",
    "    {'$limit': 5},\n",
    "]\n",
    "\n",
    "cursor = db['allcommentsinfo'].aggregate(pipeline)\n",
    "\n",
    "# Insert the selected documents into a new collection\n",
    "new_collection = db['allcommentsinfo_sample2']\n",
    "result = new_collection.insert_many(list(cursor))\n",
    "\n",
    "print(result.inserted_ids)\n",
    "\n",
    "# aggregation pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        '$unwind': {\n",
    "            'path': '$commentText',\n",
    "            'includeArrayIndex': 'arrayIndex'\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            '_id': 0,\n",
    "            'videoId': 1,\n",
    "            'commentText': 1,\n",
    "            'commentPublishedAt': {'$arrayElemAt': ['$commentPublishedAt', '$arrayIndex']},\n",
    "            'likeCount': {'$arrayElemAt': ['$likeCount', '$arrayIndex']},\n",
    "            'DataRetrievedAt': 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$out': 'allcommentsinfo_aggregated_sample2'\n",
    "    }\n",
    "]\n",
    "\n",
    "# execute aggregation pipeline\n",
    "new_collection.aggregate(pipeline)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Natural Language Sentiment Analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get sentiment analysis scores from comments collected in mongoDB using Google Cloud Natural Language API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from pymongo import MongoClient\n",
    "from google.cloud import language_v1\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['youtubedataapi0415']\n",
    "collection = db['allcommentsinfo_unwinded2']\n",
    "\n",
    "# Set up Google Cloud Natural Language API client\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "# Open CSV file for writing\n",
    "with open('sentiment0415emi.csv', mode='w', newline='') as sentiment_file:\n",
    "    sentiment_writer = csv.writer(sentiment_file)\n",
    "    sentiment_writer.writerow(\n",
    "        ['Comment', 'Language', 'LikeCount', 'CommentPublishedTime', 'Score', 'Magnitude'])\n",
    "\n",
    "    # Create a new collection for sentiment analysis results\n",
    "    sentiment_collection = db['allcommentsinfo_unwinded_sentiment2']\n",
    "\n",
    "    # Iterate over documents in collection\n",
    "    for document in collection.find():\n",
    "        # Extract commentText array\n",
    "        comment = document['commentText']\n",
    "        likecount = document['likeCount']\n",
    "        commentpublishedtime = document['commentPublishedAt']\n",
    "        # videoId = document['videoId'] (0415 no videoId)\n",
    "\n",
    "        # Check if the comment has already been analyzed\n",
    "        existing_result = sentiment_collection.find_one({'Comment': comment})\n",
    "        if existing_result:\n",
    "            print(f\"Comment '{comment}' already analyzed\")\n",
    "\n",
    "        try:\n",
    "            # Call the analyze_sentiment method to detect the language and analyze the sentiment\n",
    "            document = language_v1.Document(\n",
    "                content=comment, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
    "            response = client.analyze_sentiment(\n",
    "                request={'document': document, 'encoding_type': language_v1.EncodingType.UTF8})\n",
    "\n",
    "            # Extract language and sentiment\n",
    "            language = response.language\n",
    "            sentiment = response.document_sentiment\n",
    "\n",
    "            # Write result to CSV file\n",
    "            sentiment_writer.writerow(\n",
    "                [comment, language, likecount, commentpublishedtime, sentiment.score, sentiment.magnitude])\n",
    "\n",
    "            # Save result to MongoDB collection\n",
    "            # add videoId from allcommentsinfo unwinded collection\n",
    "            sentiment_collection.insert_one({\n",
    "                'Comment': comment,\n",
    "                'Language': language,\n",
    "                'LikeCount': likecount,\n",
    "                'CommentPublishedTime': commentpublishedtime,\n",
    "                'Score': sentiment.score,\n",
    "                'Magnitude': sentiment.magnitude\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Skip unsupported language comments\n",
    "            if 'is not supported for document_sentiment analysis' in str(e):\n",
    "                print(\n",
    "                    f\"Skipping comment '{comment}' due to unsupported language\")\n",
    "                continue\n",
    "\n",
    "            # Handle other exceptions\n",
    "            print(f\"Error processing comment '{comment}': {e}\")\n",
    "\n",
    "        # Add a 0.05-second delay between each analysis to avoid hitting the rate limit\n",
    "        time.sleep(0.05)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check and Remove duplicate comments in the sentiment collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code is to remove duplicate comments in the sentiment collection\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['youtubedataapi0412']\n",
    "collection = db['allcomments_sample_sentiment_0414']\n",
    "\n",
    "# Get a list of unique comments in the collection\n",
    "unique_comments = collection.distinct('Comment')\n",
    "\n",
    "# Iterate through the collection and remove any documents with duplicate comments\n",
    "for comment in unique_comments:\n",
    "    # Count the number of documents with this comment\n",
    "    count = collection.count_documents({'Comment': comment})\n",
    "\n",
    "    # If there is more than one document, delete all but one\n",
    "    if count > 1:\n",
    "        # Get all documents with this comment\n",
    "        documents = collection.find({'Comment': comment})\n",
    "\n",
    "        # Keep the first document and delete the rest\n",
    "        first_document = True\n",
    "        for document in documents:\n",
    "            if first_document:\n",
    "                first_document = False\n",
    "            else:\n",
    "                collection.delete_one({'_id': document['_id']})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "print(findspark.init())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/patty/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/patty/.ivy2/cache\n",
      "The jars for the packages stored in: /home/patty/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      "com.google.cloud.spark#spark-3.1-bigquery added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-360b06ed-8585-4a75-8844-128a45b1ce10;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;2.4.4 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.postgresql#postgresql;42.2.18 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "\tfound com.google.cloud.spark#spark-3.1-bigquery;0.30.0 in central\n",
      "\tfound com.google.cloud.spark#spark-bigquery-dsv2-common;0.30.0 in central\n",
      "\tfound com.google.cloud.spark#spark-bigquery-connector-common;0.30.0 in central\n",
      "\tfound com.google.cloud.spark#bigquery-connector-common;0.30.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-bigquerystorage-v1;2.34.2 in central\n",
      "\tfound io.grpc#grpc-api;1.54.0 in central\n",
      "\tfound io.grpc#grpc-context;1.54.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.54.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.54.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.15.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.54.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.22.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-bigquerystorage-v1;2.34.2 in central\n",
      "\tfound com.google.api#api-common;2.7.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound org.checkerframework#checker-qual;3.32.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.cloud#google-cloud-bigquery;2.24.4 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.14.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.22.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.10.0 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.14.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.1 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.1 in central\n",
      "\tfound com.google.api#gax-httpjson;0.109.0 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.1 in central\n",
      "\tfound com.google.http-client#google-http-client;1.43.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.5.5 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.apis#google-api-services-bigquery;v2-rev20230318-2.0.0 in central\n",
      "\tfound com.google.api#gax;2.24.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.7 in central\n",
      "\tfound org.threeten#threeten-extra;1.7.2 in central\n",
      "\tfound com.google.cloud#google-cloud-bigquerystorage;2.34.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.24.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.54.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.54.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.54.0 in central\n",
      "\tfound org.json#json;20230227 in central\n",
      "\tfound io.grpc#grpc-core;1.54.0 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.54.0 in central\n",
      "\tfound org.apache.arrow#arrow-vector;11.0.0 in central\n",
      "\tfound org.apache.arrow#arrow-format;11.0.0 in central\n",
      "\tfound com.google.flatbuffers#flatbuffers-java;1.12.0 in central\n",
      "\tfound org.apache.arrow#arrow-memory-core;11.0.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.14.2 in central\n",
      "\tfound com.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.14.2 in central\n",
      "\tfound io.netty#netty-common;4.1.90.Final in central\n",
      "\tfound com.google.inject#guice;5.1.0 in central\n",
      "\tfound javax.inject#javax.inject;1 in central\n",
      "\tfound aopalliance#aopalliance;1.0 in central\n",
      "\tfound io.grpc#grpc-netty;1.54.0 in central\n",
      "\tfound io.netty#netty-codec-http2;4.1.90.Final in central\n",
      "\tfound io.netty#netty-buffer;4.1.90.Final in central\n",
      "\tfound io.netty#netty-transport;4.1.90.Final in central\n",
      "\tfound io.netty#netty-resolver;4.1.90.Final in central\n",
      "\tfound io.netty#netty-codec;4.1.90.Final in central\n",
      "\tfound io.netty#netty-handler;4.1.90.Final in central\n",
      "\tfound io.netty#netty-transport-native-unix-common;4.1.90.Final in central\n",
      "\tfound io.netty#netty-codec-http;4.1.90.Final in central\n",
      "\tfound io.netty#netty-tcnative-boringssl-static;2.0.59.Final in central\n",
      "\tfound io.netty#netty-tcnative-classes;2.0.59.Final in central\n",
      "\tfound org.apache.arrow#arrow-memory-netty;11.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.9.1 in central\n",
      "\tfound org.apache.beam#beam-sdks-java-io-hadoop-common;2.43.0 in central\n",
      "\tfound org.apache.arrow#arrow-compression;11.0.0 in central\n",
      "\tfound org.apache.commons#commons-compress;1.22 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.54.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.23 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound io.netty#netty-handler-proxy;4.1.90.Final in central\n",
      "\tfound io.netty#netty-codec-socks;4.1.90.Final in central\n",
      ":: resolution report :: resolve 3467ms :: artifacts dl 74ms\n",
      "\t:: modules in use:\n",
      "\taopalliance#aopalliance;1.0 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.14.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.datatype#jackson-datatype-jsr310;2.14.2 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.7.0 from central in [default]\n",
      "\tcom.google.api#gax;2.24.0 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.24.0 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.109.0 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-bigquerystorage-v1;2.34.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-bigquerystorage-v1;2.34.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.15.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.10.0 from central in [default]\n",
      "\tcom.google.apis#google-api-services-bigquery;v2-rev20230318-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-bigquery;2.24.4 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-bigquerystorage;2.34.2 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.14.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.14.0 from central in [default]\n",
      "\tcom.google.cloud.spark#bigquery-connector-common;0.30.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-3.1-bigquery;0.30.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-connector-common;0.30.0 from central in [default]\n",
      "\tcom.google.cloud.spark#spark-bigquery-dsv2-common;0.30.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.9.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.flatbuffers#flatbuffers-java;1.12.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.1 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.1 from central in [default]\n",
      "\tcom.google.inject#guice;5.1.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.22.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.22.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-netty;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.54.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.54.0 from central in [default]\n",
      "\tio.netty#netty-buffer;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-codec;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-codec-http;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-codec-http2;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-codec-socks;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-common;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-handler;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-handler-proxy;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-resolver;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-boringssl-static;2.0.59.Final from central in [default]\n",
      "\tio.netty#netty-tcnative-classes;2.0.59.Final from central in [default]\n",
      "\tio.netty#netty-transport;4.1.90.Final from central in [default]\n",
      "\tio.netty#netty-transport-native-unix-common;4.1.90.Final from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjavax.inject#javax.inject;1 from central in [default]\n",
      "\torg.apache.arrow#arrow-compression;11.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-format;11.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory-core;11.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-memory-netty;11.0.0 from central in [default]\n",
      "\torg.apache.arrow#arrow-vector;11.0.0 from central in [default]\n",
      "\torg.apache.beam#beam-sdks-java-io-hadoop-common;2.43.0 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.22 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;2.4.4 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.5.5 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.32.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.23 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.json#json;20230227 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.postgresql#postgresql;42.2.18 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.threeten#threeten-extra;1.7.2 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.7 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;3.5.0 by [org.checkerframework#checker-qual;3.32.0] in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 by [com.google.code.gson#gson;2.9.1] in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 by [org.checkerframework#checker-qual;3.32.0] in [default]\n",
      "\torg.checkerframework#checker-qual;3.12.0 by [org.checkerframework#checker-qual;3.32.0] in [default]\n",
      "\tcom.google.code.gson#gson;2.9.0 by [com.google.code.gson#gson;2.9.1] in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.21 by [org.codehaus.mojo#animal-sniffer-annotations;1.23] in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 by [io.perfmark#perfmark-api;0.26.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  110  |   0   |   0   |   7   ||  103  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-360b06ed-8585-4a75-8844-128a45b1ce10\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 103 already retrieved (0kB/31ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 03:57:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setup pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "packages = [\n",
    "    \"org.apache.hadoop:hadoop-aws:3.2.0\",\n",
    "    \"org.apache.spark:spark-avro_2.12:2.4.4\",\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
    "    \"org.postgresql:postgresql:42.2.18\",\n",
    "    \"com.google.cloud.spark:spark-3.1-bigquery:0.30.0\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Transform youtube video data\")\\\n",
    "    .master('spark://localhost:7077')\\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from mongoDB to spark\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of top5hkchannelsinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ChannelTitle: string (nullable = true)\n",
      " |-- DataRetrievedAt: string (nullable = true)\n",
      " |-- channelId: string (nullable = true)\n",
      " |-- channelPublishedAt: string (nullable = true)\n",
      " |-- playlistId: string (nullable = true)\n",
      " |-- subscribers: integer (nullable = true)\n",
      " |-- totalVideos: integer (nullable = true)\n",
      " |-- totalViews: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+----------+\n",
      "| ChannelTitle|     DataRetrievedAt|           channelId|  channelPublishedAt|          playlistId|subscribers|totalVideos|totalViews|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+----------+\n",
      "|       笑波子|2023-04-16T09:38:...|UCDpK1rg5I9Zc3ToY...|2006-09-09T19:59:59Z|UUDpK1rg5I9Zc3ToY...|     936000|       4354|1051897493|\n",
      "|        JASON|2023-04-16T09:38:...|UC4nsi0oM9WBNFv1R...|2013-06-16T13:50:59Z|UU4nsi0oM9WBNFv1R...|    1040000|       2955| 521248369|\n",
      "|點 Cook Guide|2023-04-16T09:38:...|UCXnWjmQ8BDE0sDIe...|2014-02-07T15:44:04Z|UUXnWjmQ8BDE0sDIe...|    1110000|       1356| 202603292|\n",
      "|     emi wong|2023-04-16T09:38:...|UCvGEK5_U-kLgO6-A...|2014-11-02T14:43:34Z|UUvGEK5_U-kLgO6-A...|    5960000|        442| 787128269|\n",
      "| Coffee林芊妤|2023-04-16T09:38:...|UCxCZqbizSsnntlz6...|2015-06-02T07:09:15Z|UUxCZqbizSsnntlz6...|    1700000|        353| 261803319|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_top5hkchannelsinfo_0415 = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0416.top5hkchannelsinfo').load()\n",
    "df_top5hkchannelsinfo_0415 = df_top5hkchannelsinfo_0415.drop('_id')\n",
    "df_top5hkchannelsinfo_0415.dtypes\n",
    "df_top5hkchannelsinfo_0415.printSchema()\n",
    "df_top5hkchannelsinfo_0415.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of allvideodetails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- Caption: string (nullable = true)\n",
      " |-- ChannelTitle: string (nullable = true)\n",
      " |-- Comments: integer (nullable = true)\n",
      " |-- DataRetrievedAt: string (nullable = true)\n",
      " |-- Definition: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Duration: double (nullable = true)\n",
      " |-- Likes: integer (nullable = true)\n",
      " |-- Published_date: string (nullable = true)\n",
      " |-- Tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- VideoId: string (nullable = true)\n",
      " |-- VideoTitle: string (nullable = true)\n",
      " |-- Views: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+\n",
      "|year|month|day|hour|  weekday|Caption|ChannelTitle|Comments|     DataRetrievedAt|Definition|                          Description|Duration|Likes|      Published_date|                     Tags|    VideoId|                         VideoTitle| Views|\n",
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+\n",
      "|2023|    4| 15|   9| Saturday|   true|      笑波子|      90|2023-04-16T09:38:...|        hd|    無人能抗拒這麼可愛的肉桂狗Café...|   566.0| 1342|2023-04-15T09:06:10Z|[笑波子, 波子, hk, vlo...|yOwSdw2Ep60|     【😍肉桂狗主題Café 】把肉桂...| 33366|\n",
      "|2023|    4| 15|   3| Saturday|  false|      笑波子|      65|2023-04-16T09:38:...|        hd|  為了林婆婆😳蔬菜歌唱組合成立啦！...|    45.0|  988|2023-04-15T03:21:58Z|                     null|XyWlV_5wE6U|  林婆婆沒有錯👵🏻！生菜白菜是歌...| 15467|\n",
      "|2023|    4| 14|   9|   Friday|   true|      笑波子|      43|2023-04-16T09:38:...|        hd|      從日本京都🇯🇵買來的美食\\n抹...|   529.0|  985|2023-04-14T09:30:10Z|[笑波子, 波子, hk, vlo...|_Kacv5GlJxg|  【從日本京都🇯🇵買來的美食】抹...| 27302|\n",
      "|2023|    4| 14|   9|   Friday|  false|      笑波子|      36|2023-04-16T09:38:...|        hd|          太空打地鼠？打Imposter? ...|    23.0|  441|2023-04-14T09:01:39Z|                     null|e1WRjaYgKJE|       玩過最惡搞的Among Us😳？？？|  9052|\n",
      "|2023|    4| 13|  11| Thursday|   true|      笑波子|      42|2023-04-16T09:38:...|        hd|  😛幾年後再來這間夾娃娃店！弘大的...|   380.0| 1333|2023-04-13T11:01:15Z|[笑波子, 波子, hk, vlo...|EFXej9P6xk4| 【😛韓國夾娃娃天堂】疫情後再來…...| 49480|\n",
      "|2023|    4| 12|   9|Wednesday|  false|      笑波子|      38|2023-04-16T09:38:...|        hd|           當Among Us變成真人化😈?...|   631.0|  577|2023-04-12T09:00:34Z|[笑波子, 波子, hk, vlo...|o-lIiyrzy38|      Among Us真人化？宇宙狼人殺...| 20068|\n",
      "|2023|    4| 11|  12|  Tuesday|   true|      笑波子|      90|2023-04-16T09:38:...|        hd|如果我在香港「做這件事」便犯法了？...|   485.0| 2097|2023-04-11T12:18:12Z|[笑波子, 波子, hk, vlo...|SFSJEZcFLsM|     我做了「😲在香港犯法🈲」......|103896|\n",
      "|2023|    4| 11|  11|  Tuesday|  false|      笑波子|      13|2023-04-16T09:38:...|        hd|               Doge simulator 柴狗...|  7375.0|  407|2023-04-11T11:08:19Z|[笑波子, 波子, hk, vlo...|sn0m4w66ubE|     【狗狗版GTA🐶】爆笑柴犬🤣炸...| 15409|\n",
      "|2023|    4| 10|  10|   Monday|  false|      笑波子|     146|2023-04-16T09:38:...|        hd|      星期日好多姐姐街上跳舞💃\\n😳...|    60.0| 4142|2023-04-10T10:18:26Z|                     null|cc1BS62lZC0|【街頭挑戰跟姐姐跳舞💃】叮叮噹噹...| 67203|\n",
      "|2023|    4| 10|   9|   Monday|  false|      笑波子|       6|2023-04-16T09:38:...|        hd|             加班台上: https://www...| 17712.0|  287|2023-04-10T09:26:57Z|[笑波子, 波子, hk, vlo...|DIBlgkT-xPk|  【復活節加班台(下)】😱直播到天...| 12294|\n",
      "|2023|    4| 10|   7|   Monday|  false|      笑波子|      66|2023-04-16T09:38:...|        hd|   啪啪啪之神再臨😜 又可以早午晚食...|   555.0| 1807|2023-04-10T07:41:39Z|[笑波子, 波子, hk, vlo...|nM-ZWw3e5cc|   【夾娃娃】🇭🇰限定「香港之光?...| 75475|\n",
      "|2023|    4|  9|  20|   Sunday|  false|      笑波子|      14|2023-04-16T09:38:...|        hd|           金主DONATE👉🏻(會自動讀...| 15169.0|  362|2023-04-09T20:47:00Z|[笑波子, 波子, hk, vlo...|8shhIhsX3hg|    【鵝鴨殺!!!!】😳生眼挑針都要...| 16111|\n",
      "|2023|    4|  8|   9| Saturday|   true|      笑波子|     128|2023-04-16T09:38:...|        hd|  波子一日店長系列又來啦！\\n好開心...|   962.0| 2669|2023-04-08T09:00:00Z|[笑波子, 波子, hk, vlo...|-vvZF507BVE|       【笑波子再㊙️撈👨🏻‍💼】 ...| 84002|\n",
      "|2023|    4|  7|  20|   Friday|  false|      笑波子|      18|2023-04-16T09:38:...|        hd|           金主DONATE👉🏻(會自動讀...| 21367.0|  513|2023-04-07T20:08:06Z|[笑波子, 波子, hk, vlo...|0F3_re_Cno0|【復活節加班台】😆你猜我畫「復活...| 23161|\n",
      "|2023|    4|  7|   8|   Friday|   true|      笑波子|     162|2023-04-16T09:38:...|        hd|    第一個除罩後的掃街！ (**按設定...|   994.0| 2864|2023-04-07T08:26:07Z|[笑波子, 波子, hk, vlo...|oVpnGKHy7qs|    【😛葵廣美食掃街2023】食盡新...|135966|\n",
      "|2023|    4|  6|   8| Thursday|  false|      笑波子|      74|2023-04-16T09:38:...|        hd|  閉店裝修一個多月後🔍有什麼變化？...|   497.0| 1575|2023-04-06T08:54:04Z|[笑波子, 波子, hk, vlo...|myNf93uFPp4|     【夾娃娃】朗豪坊Namco重開🤩...| 69080|\n",
      "|2023|    4|  5|   8|Wednesday|  false|      笑波子|      46|2023-04-16T09:38:...|        hd|            笨笨的死法DUMB WAYS TO...|   696.0| 1043|2023-04-05T08:51:52Z|[笑波子, 波子, hk, vlo...|_lI7y40LbFA|【😨阻止朋友自殺的遊戲？！】可是...| 35960|\n",
      "|2023|    4|  4|   7|  Tuesday|  false|      笑波子|      70|2023-04-16T09:38:...|        hd|   完成100萬訂閱笨豬跳之前，第一次...|   639.0| 1450|2023-04-04T07:15:06Z|[笑波子, 波子, hk, vlo...|ExFuS-VSE0M|   【極限挑戰🔥】我在600米高山上...| 45426|\n",
      "|2023|    4|  3|   8|   Monday|   true|      笑波子|      85|2023-04-16T09:38:...|        hd|    單色夾娃娃挑戰又來啦！🌈\\n大家...|   351.0| 1326|2023-04-03T08:31:31Z|[笑波子, 波子, hk, vlo...|KC45-mFfhyQ|   【夾娃娃】 💛顏色挑戰🌝把所有...| 55034|\n",
      "|2023|    4|  2|  17|   Sunday|  false|      笑波子|       5|2023-04-16T09:38:...|        hd|               Hardcore~XD 仲係Har...|  7350.0|  292|2023-04-02T17:06:27Z|[笑波子, 波子, hk, vlo...|H6qNZ6ZFxxI|             【Resident Evil 4最...| 11872|\n",
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_allvideodetails0418 = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0416.allvideodetails').load()\n",
    "\n",
    "df_allvideodetails0418.createOrReplaceTempView('df_allvideodetails0418')\n",
    "\n",
    "df_allvideodetails0418 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(year from df_allvideodetails0418.Published_date) as year,\n",
    "        EXTRACT(month from df_allvideodetails0418.Published_date) as month,\n",
    "        EXTRACT(day from df_allvideodetails0418.Published_date) as day,\n",
    "        EXTRACT(hour from df_allvideodetails0418.Published_date) as hour,\n",
    "        date_format(df_allvideodetails0418.Published_date, 'EEEE') as weekday,\n",
    "        *\n",
    "    FROM df_allvideodetails0418\n",
    "\"\"\")\n",
    "df_allvideodetails0418 = df_allvideodetails0418.drop(\"_id\")\n",
    "df_allvideodetails0418.printSchema()\n",
    "df_allvideodetails0418.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calulating ratio of likes/comments/duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- Caption: string (nullable = true)\n",
      " |-- ChannelTitle: string (nullable = true)\n",
      " |-- Comments: integer (nullable = true)\n",
      " |-- DataRetrievedAt: string (nullable = true)\n",
      " |-- Definition: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Duration: double (nullable = true)\n",
      " |-- Likes: integer (nullable = true)\n",
      " |-- Published_date: string (nullable = true)\n",
      " |-- Tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- VideoId: string (nullable = true)\n",
      " |-- VideoTitle: string (nullable = true)\n",
      " |-- Views: integer (nullable = true)\n",
      " |-- TagNum: integer (nullable = false)\n",
      " |-- likeRatio: double (nullable = true)\n",
      " |-- commentRatio: double (nullable = true)\n",
      "\n",
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+------+--------------------+--------------------+\n",
      "|year|month|day|hour|  weekday|Caption|ChannelTitle|Comments|     DataRetrievedAt|Definition|                          Description|Duration|Likes|      Published_date|                     Tags|    VideoId|                         VideoTitle| Views|TagNum|           likeRatio|        commentRatio|\n",
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+------+--------------------+--------------------+\n",
      "|2023|    4| 15|   9| Saturday|   true|      笑波子|      90|2023-04-16T09:38:...|        hd|    無人能抗拒這麼可愛的肉桂狗Café...|   566.0| 1342|2023-04-15T09:06:10Z|[笑波子, 波子, hk, vlo...|yOwSdw2Ep60|     【😍肉桂狗主題Café 】把肉桂...| 33366|     7| 0.04022058382784871|0.002697356590541...|\n",
      "|2023|    4| 15|   3| Saturday|  false|      笑波子|      65|2023-04-16T09:38:...|        hd|  為了林婆婆😳蔬菜歌唱組合成立啦！...|    45.0|  988|2023-04-15T03:21:58Z|                       []|XyWlV_5wE6U|  林婆婆沒有錯👵🏻！生菜白菜是歌...| 15467|     0| 0.06387793366522274|0.004202495635869916|\n",
      "|2023|    4| 14|   9|   Friday|   true|      笑波子|      43|2023-04-16T09:38:...|        hd|      從日本京都🇯🇵買來的美食\\n抹...|   529.0|  985|2023-04-14T09:30:10Z|[笑波子, 波子, hk, vlo...|_Kacv5GlJxg|  【從日本京都🇯🇵買來的美食】抹...| 27302|     7| 0.03607794300783825|0.001574976192220...|\n",
      "|2023|    4| 14|   9|   Friday|  false|      笑波子|      36|2023-04-16T09:38:...|        hd|          太空打地鼠？打Imposter? ...|    23.0|  441|2023-04-14T09:01:39Z|                       []|e1WRjaYgKJE|       玩過最惡搞的Among Us😳？？？|  9052|     0| 0.04871851524524967|0.003977021652673442|\n",
      "|2023|    4| 13|  11| Thursday|   true|      笑波子|      42|2023-04-16T09:38:...|        hd|  😛幾年後再來這間夾娃娃店！弘大的...|   380.0| 1333|2023-04-13T11:01:15Z|[笑波子, 波子, hk, vlo...|EFXej9P6xk4| 【😛韓國夾娃娃天堂】疫情後再來…...| 49480|     7|0.026940177849636218|8.488278092158447E-4|\n",
      "|2023|    4| 12|   9|Wednesday|  false|      笑波子|      38|2023-04-16T09:38:...|        hd|           當Among Us變成真人化😈?...|   631.0|  577|2023-04-12T09:00:34Z|[笑波子, 波子, hk, vlo...|o-lIiyrzy38|      Among Us真人化？宇宙狼人殺...| 20068|     7|0.028752242375921867|0.001893561889575...|\n",
      "|2023|    4| 11|  12|  Tuesday|   true|      笑波子|      90|2023-04-16T09:38:...|        hd|如果我在香港「做這件事」便犯法了？...|   485.0| 2097|2023-04-11T12:18:12Z|[笑波子, 波子, hk, vlo...|SFSJEZcFLsM|     我做了「😲在香港犯法🈲」......|103896|     7|0.020183645183645182|8.662508662508662E-4|\n",
      "|2023|    4| 11|  11|  Tuesday|  false|      笑波子|      13|2023-04-16T09:38:...|        hd|               Doge simulator 柴狗...|  7375.0|  407|2023-04-11T11:08:19Z|[笑波子, 波子, hk, vlo...|sn0m4w66ubE|     【狗狗版GTA🐶】爆笑柴犬🤣炸...| 15409|    12| 0.02641313518073853|8.436627944707638E-4|\n",
      "|2023|    4| 10|  10|   Monday|  false|      笑波子|     146|2023-04-16T09:38:...|        hd|      星期日好多姐姐街上跳舞💃\\n😳...|    60.0| 4142|2023-04-10T10:18:26Z|                       []|cc1BS62lZC0|【街頭挑戰跟姐姐跳舞💃】叮叮噹噹...| 67203|     0|0.061634153237206675|0.002172522060027082|\n",
      "|2023|    4| 10|   9|   Monday|  false|      笑波子|       6|2023-04-16T09:38:...|        hd|             加班台上: https://www...| 17712.0|  287|2023-04-10T09:26:57Z|[笑波子, 波子, hk, vlo...|DIBlgkT-xPk|  【復活節加班台(下)】😱直播到天...| 12294|    12|0.023344721002114852|4.880429477794046E-4|\n",
      "|2023|    4| 10|   7|   Monday|  false|      笑波子|      66|2023-04-16T09:38:...|        hd|   啪啪啪之神再臨😜 又可以早午晚食...|   555.0| 1807|2023-04-10T07:41:39Z|[笑波子, 波子, hk, vlo...|nM-ZWw3e5cc|   【夾娃娃】🇭🇰限定「香港之光?...| 75475|     7|0.023941702550513415|8.744617422987744E-4|\n",
      "|2023|    4|  9|  20|   Sunday|  false|      笑波子|      14|2023-04-16T09:38:...|        hd|           金主DONATE👉🏻(會自動讀...| 15169.0|  362|2023-04-09T20:47:00Z|[笑波子, 波子, hk, vlo...|8shhIhsX3hg|    【鵝鴨殺!!!!】😳生眼挑針都要...| 16111|    12|0.022469120476692944|8.689715101483458E-4|\n",
      "|2023|    4|  8|   9| Saturday|   true|      笑波子|     128|2023-04-16T09:38:...|        hd|  波子一日店長系列又來啦！\\n好開心...|   962.0| 2669|2023-04-08T09:00:00Z|[笑波子, 波子, hk, vlo...|-vvZF507BVE|       【笑波子再㊙️撈👨🏻‍💼】 ...| 84002|     7|0.031773053022547085|0.001523773243494...|\n",
      "|2023|    4|  7|  20|   Friday|  false|      笑波子|      18|2023-04-16T09:38:...|        hd|           金主DONATE👉🏻(會自動讀...| 21367.0|  513|2023-04-07T20:08:06Z|[笑波子, 波子, hk, vlo...|0F3_re_Cno0|【復活節加班台】😆你猜我畫「復活...| 23161|    12|0.022149302707136997|7.771685160398947E-4|\n",
      "|2023|    4|  7|   8|   Friday|   true|      笑波子|     162|2023-04-16T09:38:...|        hd|    第一個除罩後的掃街！ (**按設定...|   994.0| 2864|2023-04-07T08:26:07Z|[笑波子, 波子, hk, vlo...|oVpnGKHy7qs|    【😛葵廣美食掃街2023】食盡新...|135966|     7|0.021064089551799715|0.001191474339173...|\n",
      "|2023|    4|  6|   8| Thursday|  false|      笑波子|      74|2023-04-16T09:38:...|        hd|  閉店裝修一個多月後🔍有什麼變化？...|   497.0| 1575|2023-04-06T08:54:04Z|[笑波子, 波子, hk, vlo...|myNf93uFPp4|     【夾娃娃】朗豪坊Namco重開🤩...| 69080|     7|0.022799652576722642|0.001071221771858...|\n",
      "|2023|    4|  5|   8|Wednesday|  false|      笑波子|      46|2023-04-16T09:38:...|        hd|            笨笨的死法DUMB WAYS TO...|   696.0| 1043|2023-04-05T08:51:52Z|[笑波子, 波子, hk, vlo...|_lI7y40LbFA|【😨阻止朋友自殺的遊戲？！】可是...| 35960|     7|0.029004449388209123|0.001279199110122...|\n",
      "|2023|    4|  4|   7|  Tuesday|  false|      笑波子|      70|2023-04-16T09:38:...|        hd|   完成100萬訂閱笨豬跳之前，第一次...|   639.0| 1450|2023-04-04T07:15:06Z|[笑波子, 波子, hk, vlo...|ExFuS-VSE0M|   【極限挑戰🔥】我在600米高山上...| 45426|     7| 0.03192004578875534|0.001540967727733...|\n",
      "|2023|    4|  3|   8|   Monday|   true|      笑波子|      85|2023-04-16T09:38:...|        hd|    單色夾娃娃挑戰又來啦！🌈\\n大家...|   351.0| 1326|2023-04-03T08:31:31Z|[笑波子, 波子, hk, vlo...|KC45-mFfhyQ|   【夾娃娃】 💛顏色挑戰🌝把所有...| 55034|     7| 0.02409419631500527|0.001544499763782389|\n",
      "|2023|    4|  2|  17|   Sunday|  false|      笑波子|       5|2023-04-16T09:38:...|        hd|               Hardcore~XD 仲係Har...|  7350.0|  292|2023-04-02T17:06:27Z|[笑波子, 波子, hk, vlo...|H6qNZ6ZFxxI|             【Resident Evil 4最...| 11872|    12|0.024595687331536387|4.211590296495957E-4|\n",
      "+----+-----+---+----+---------+-------+------------+--------+--------------------+----------+-------------------------------------+--------+-----+--------------------+-------------------------+-----------+-----------------------------------+------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size, array, when, col\n",
    "\n",
    "\n",
    "# Use when() function to replace null values with empty arrays\n",
    "df_allvideodetails0418 = df_allvideodetails0418.withColumn(\"Tags\", when(\n",
    "    df_allvideodetails0418.Tags.isNull(), array()).otherwise(df_allvideodetails0418.Tags))\n",
    "\n",
    "# Add a new column with array length\n",
    "df_allvideodetails0418 = df_allvideodetails0418.withColumn(\n",
    "    \"TagNum\", size(df_allvideodetails0418.Tags))\n",
    "\n",
    "# Replace -1 values with 0 in TagNum column\n",
    "df_allvideodetails0418 = df_allvideodetails0418.withColumn(\"TagNum\", when(\n",
    "    df_allvideodetails0418.TagNum == -1, 0).otherwise(df_allvideodetails0418.TagNum))\n",
    "\n",
    "# Add a new column 'likeRatio' with calculated values\n",
    "df_allvideodetails0418 = df_allvideodetails0418.withColumn(\"likeRatio\", col(\"Likes\") / col(\"Views\"))\n",
    "\n",
    "# Add a new column 'commentRatio' with calculated values\n",
    "df_allvideodetails0418 = df_allvideodetails0418.withColumn(\"commentRatio\", col(\"Comments\") / col(\"Views\"))\n",
    "\n",
    "# print the schema\n",
    "df_allvideodetails0418.printSchema()\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_allvideodetails0418.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of allvideocomments without sentiment scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|year|month|day|hour|  weekday|     DataRetrievedAt|                 _id|  commentPublishedAt|                          commentText|likeCount|    videoId|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-07T04:23:51Z|                           长大了许多|        0|WxTxeyqRM4E|\n",
      "|2023|    4|  6|  23| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-06T23:17:43Z|                           So cute❤~~|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  5|   7|Wednesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-05T07:42:41Z|                   最可愛的代言人😄🥰|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  14|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T14:00:06Z|                  Coffee Sweet 好得意|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  11|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T11:33:17Z|                         仔仔好得意😂|        4|WxTxeyqRM4E|\n",
      "|2023|    4| 15|   3| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-15T03:50:00Z|       真的希望可以keep住每日做⋯⋯?...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 13|   1| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-13T01:07:21Z|   能出髖關節伸展瑜伽嗎～？ \\n久坐...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 11|  13|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-11T13:21:17Z|                         So relaxing❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 10|  19|   Monday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-10T19:11:18Z|                          大愛呢條片❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   2|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T02:00:22Z|                 Happy Easter 🐰🐣...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:59:54Z|                 Thank you for sha...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:59:26Z|               好開心陪寶貝公主做瑜伽|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:53:45Z|早上一起做完一組後，剛睡醒的緊繃感...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-08T07:54:13Z|   最近太忙沒有好好認真拉筋\\n看到c...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-08T07:51:13Z|                 Challenging for m...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-07T04:18:21Z|       coffee姐姐 請求下次可以出一...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  6|  14| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-06T14:21:48Z|         嗨 coffee 老师 有没有睡前...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T23:37:15Z|         Coffee 好久没出背部训练了 😳|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T23:10:42Z|不知道老師可不可以在每一個動作的剛...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  21|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T21:41:49Z|             可以出肩頸拉伸的瑜伽嗎？|        0|ibZ6t_lTTIA|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+-----+---+----+--------+--------------------+--------------------+--------------------+---------------------+---------+-----------+\n",
      "|year|month|day|hour| weekday|     DataRetrievedAt|                 _id|  commentPublishedAt|          commentText|likeCount|    videoId|\n",
      "+----+-----+---+----+--------+--------------------+--------------------+--------------------+---------------------+---------+-----------+\n",
      "|2023|    4| 15|   9|Saturday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-15T09:53:26Z| Do you have exerc...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 15|   9|Saturday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-15T09:37:06Z|         thanks girl!|        0|pgGoBihIUiU|\n",
      "|2023|    4| 15|   1|Saturday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-15T01:20:47Z|             Love it❤|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  23|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T23:53:37Z|                   Me|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  19|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T19:45:52Z| Yesss pleaseee mo...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  19|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T19:21:14Z|               𝕎𝕆𝕎|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  19|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T19:20:59Z|                  Wow|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  15|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T15:31:15Z| Hi emi. Please Ki...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|  14|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T14:37:14Z| We need more of t...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|   9|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T09:15:06Z| I enjoyed this wo...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|   2|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T02:50:17Z| Wow done maam\\nTh...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 14|   2|  Friday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-14T02:05:41Z| Who's sitting whi...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|  16|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T16:44:07Z| I like your all v...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|  15|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T15:55:39Z| Back to excercise...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|  12|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T12:52:36Z| thank you emi iam...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|  11|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T11:29:48Z| Hi,Emi!♡I like yo...|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|   9|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T09:47:31Z| Omg 🎉🎉🎉....you...|        1|pgGoBihIUiU|\n",
      "|2023|    4| 13|   7|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T07:36:56Z|         Lost of fun.|        0|pgGoBihIUiU|\n",
      "|2023|    4| 13|   6|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T06:27:23Z|You’re so cute～ L...|        1|pgGoBihIUiU|\n",
      "|2023|    4| 13|   4|Thursday|2023-04-15T11:16:...|{643a8880332a3928...|2023-04-13T04:42:20Z| Morning workout b...|        1|pgGoBihIUiU|\n",
      "+----+-----+---+----+--------+--------------------+--------------------+--------------------+---------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_allcommentsinfo_unwinded0415_coffee = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0415.allcommentsinfo_unwinded').load()\n",
    "\n",
    "df_allcommentsinfo_unwinded0415_emi = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0415.allcommentsinfo_unwinded2').load()\n",
    "\n",
    "df_allcommentsinfo_unwinded0415_coffee.createOrReplaceTempView(\n",
    "    'df_allcommentsinfo_unwinded0415_coffee')\n",
    "df_allcommentsinfo_unwinded0415_emi.createOrReplaceTempView(\n",
    "    'df_allcommentsinfo_unwinded0415_emi')\n",
    "\n",
    "df_allcommentsinfo_unwinded0415_coffee = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(year from df_allcommentsinfo_unwinded0415_coffee.commentPublishedAt) as year,\n",
    "        EXTRACT(month from df_allcommentsinfo_unwinded0415_coffee.commentPublishedAt) as month,\n",
    "        EXTRACT(day from df_allcommentsinfo_unwinded0415_coffee.commentPublishedAt) as day,\n",
    "        EXTRACT(hour from df_allcommentsinfo_unwinded0415_coffee.commentPublishedAt ) as hour,\n",
    "        date_format(df_allcommentsinfo_unwinded0415_coffee.commentPublishedAt, 'EEEE') as weekday,\n",
    "        *\n",
    "    FROM df_allcommentsinfo_unwinded0415_coffee\n",
    "\"\"\")\n",
    "\n",
    "df_allcommentsinfo_unwinded0415_emi = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        EXTRACT(year from df_allcommentsinfo_unwinded0415_emi.commentPublishedAt) as year,\n",
    "        EXTRACT(month from df_allcommentsinfo_unwinded0415_emi.commentPublishedAt) as month,\n",
    "        EXTRACT(day from df_allcommentsinfo_unwinded0415_emi.commentPublishedAt) as day,\n",
    "        EXTRACT(hour from df_allcommentsinfo_unwinded0415_emi.commentPublishedAt ) as hour,\n",
    "        date_format(df_allcommentsinfo_unwinded0415_emi.commentPublishedAt, 'EEEE') as weekday,\n",
    "        *\n",
    "    FROM df_allcommentsinfo_unwinded0415_emi\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "df_allcommentsinfo_unwinded0415_coffee.show()\n",
    "df_allcommentsinfo_unwinded0415_emi.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|year|month|day|hour|  weekday|     DataRetrievedAt|                 _id|  commentPublishedAt|                          commentText|likeCount|    videoId|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-07T04:23:51Z|                           长大了许多|        0|WxTxeyqRM4E|\n",
      "|2023|    4|  6|  23| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-06T23:17:43Z|                           So cute❤~~|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  5|   7|Wednesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-05T07:42:41Z|                   最可愛的代言人😄🥰|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  14|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T14:00:06Z|                  Coffee Sweet 好得意|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  11|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T11:33:17Z|                         仔仔好得意😂|        4|WxTxeyqRM4E|\n",
      "|2023|    4| 15|   3| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-15T03:50:00Z|       真的希望可以keep住每日做⋯⋯?...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 13|   1| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-13T01:07:21Z|   能出髖關節伸展瑜伽嗎～？ \\n久坐...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 11|  13|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-11T13:21:17Z|                         So relaxing❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 10|  19|   Monday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-10T19:11:18Z|                          大愛呢條片❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   2|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T02:00:22Z|                 Happy Easter 🐰🐣...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:59:54Z|                 Thank you for sha...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:59:26Z|               好開心陪寶貝公主做瑜伽|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-09T01:53:45Z|早上一起做完一組後，剛睡醒的緊繃感...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-08T07:54:13Z|   最近太忙沒有好好認真拉筋\\n看到c...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-08T07:51:13Z|                 Challenging for m...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-07T04:18:21Z|       coffee姐姐 請求下次可以出一...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  6|  14| Thursday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-06T14:21:48Z|         嗨 coffee 老师 有没有睡前...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T23:37:15Z|         Coffee 好久没出背部训练了 😳|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T23:10:42Z|不知道老師可不可以在每一個動作的剛...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  21|  Tuesday|2023-04-15T07:07:...|{643a546c332a3928...|2023-04-04T21:41:49Z|             可以出肩頸拉伸的瑜伽嗎？|        0|ibZ6t_lTTIA|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# merge the dataframes using union\n",
    "merged_df_allcommentsinfo_unwinded = df_allcommentsinfo_unwinded0415_coffee.union(\n",
    "    df_allcommentsinfo_unwinded0415_emi)\n",
    "\n",
    "# show the merged dataframe\n",
    "merged_df_allcommentsinfo_unwinded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_allcommentsinfo_unwinded = merged_df_allcommentsinfo_unwinded.withColumnRenamed(\n",
    "    \"commentText\", \"Comment\")\n",
    "merged_df_allcommentsinfo_unwinded = merged_df_allcommentsinfo_unwinded.withColumnRenamed(\n",
    "    \"likeCount\", \"LikeCount\")\n",
    "merged_df_allcommentsinfo_unwinded = merged_df_allcommentsinfo_unwinded.withColumnRenamed(\n",
    "    \"commentPublishedAt\", \"CommentPublishedTime\")\n",
    "merged_df_allcommentsinfo_unwinded = merged_df_allcommentsinfo_unwinded.drop(\n",
    "    \"_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|year|month|day|hour|  weekday|     DataRetrievedAt|CommentPublishedTime|                              Comment|LikeCount|    videoId|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|2023-04-07T04:23:51Z|                           长大了许多|        0|WxTxeyqRM4E|\n",
      "|2023|    4|  6|  23| Thursday|2023-04-15T07:07:...|2023-04-06T23:17:43Z|                           So cute❤~~|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  5|   7|Wednesday|2023-04-15T07:07:...|2023-04-05T07:42:41Z|                   最可愛的代言人😄🥰|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  14|  Tuesday|2023-04-15T07:07:...|2023-04-04T14:00:06Z|                  Coffee Sweet 好得意|        2|WxTxeyqRM4E|\n",
      "|2023|    4|  4|  11|  Tuesday|2023-04-15T07:07:...|2023-04-04T11:33:17Z|                         仔仔好得意😂|        4|WxTxeyqRM4E|\n",
      "|2023|    4| 15|   3| Saturday|2023-04-15T07:07:...|2023-04-15T03:50:00Z|       真的希望可以keep住每日做⋯⋯?...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 13|   1| Thursday|2023-04-15T07:07:...|2023-04-13T01:07:21Z|   能出髖關節伸展瑜伽嗎～？ \\n久坐...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 11|  13|  Tuesday|2023-04-15T07:07:...|2023-04-11T13:21:17Z|                         So relaxing❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4| 10|  19|   Monday|2023-04-15T07:07:...|2023-04-10T19:11:18Z|                          大愛呢條片❤|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   2|   Sunday|2023-04-15T07:07:...|2023-04-09T02:00:22Z|                 Happy Easter 🐰🐣...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|2023-04-09T01:59:54Z|                 Thank you for sha...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|2023-04-09T01:59:26Z|               好開心陪寶貝公主做瑜伽|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  9|   1|   Sunday|2023-04-15T07:07:...|2023-04-09T01:53:45Z|早上一起做完一組後，剛睡醒的緊繃感...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|2023-04-08T07:54:13Z|   最近太忙沒有好好認真拉筋\\n看到c...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  8|   7| Saturday|2023-04-15T07:07:...|2023-04-08T07:51:13Z|                 Challenging for m...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  7|   4|   Friday|2023-04-15T07:07:...|2023-04-07T04:18:21Z|       coffee姐姐 請求下次可以出一...|        1|ibZ6t_lTTIA|\n",
      "|2023|    4|  6|  14| Thursday|2023-04-15T07:07:...|2023-04-06T14:21:48Z|         嗨 coffee 老师 有没有睡前...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|2023-04-04T23:37:15Z|         Coffee 好久没出背部训练了 😳|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  23|  Tuesday|2023-04-15T07:07:...|2023-04-04T23:10:42Z|不知道老師可不可以在每一個動作的剛...|        0|ibZ6t_lTTIA|\n",
      "|2023|    4|  4|  21|  Tuesday|2023-04-15T07:07:...|2023-04-04T21:41:49Z|             可以出肩頸拉伸的瑜伽嗎？|        0|ibZ6t_lTTIA|\n",
      "+----+-----+---+----+---------+--------------------+--------------------+-------------------------------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df_allcommentsinfo_unwinded.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of allvideocomments with sentiment scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------+---------+-------------------+--------------------+\n",
      "|              Comment|CommentPublishedTime|Language|LikeCount|          Magnitude|               Score|\n",
      "+---------------------+--------------------+--------+---------+-------------------+--------------------+\n",
      "| Do you have exerc...|2023-04-15T09:53:26Z|      en|        0|0.10000000149011612|-0.10000000149011612|\n",
      "|         thanks girl!|2023-04-15T09:37:06Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "|             Love it❤|2023-04-15T01:20:47Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "|                   Me|2023-04-14T23:53:37Z|      en|        0|0.30000001192092896| 0.30000001192092896|\n",
      "| Yesss pleaseee mo...|2023-04-14T19:45:52Z|      en|        0| 0.4000000059604645|  0.4000000059604645|\n",
      "|               𝕎𝕆𝕎|2023-04-14T19:21:14Z|      en|        0|0.30000001192092896| 0.30000001192092896|\n",
      "|                  Wow|2023-04-14T19:20:59Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "| Hi emi. Please Ki...|2023-04-14T15:31:15Z|      en|        0|0.30000001192092896| 0.10000000149011612|\n",
      "| We need more of t...|2023-04-14T14:37:14Z|      en|        0|  0.800000011920929|   0.800000011920929|\n",
      "| I enjoyed this wo...|2023-04-14T09:15:06Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "| Wow done maam\\nTh...|2023-04-14T02:50:17Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "| Who's sitting whi...|2023-04-14T02:05:41Z|      en|        0|                0.5|                 0.0|\n",
      "| I like your all v...|2023-04-13T16:44:07Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "| Back to excercise...|2023-04-13T15:55:39Z|      en|        0| 0.8999999761581421|  0.8999999761581421|\n",
      "| thank you emi iam...|2023-04-13T12:52:36Z|      en|        0|  1.100000023841858|                 0.5|\n",
      "| Hi,Emi!♡I like yo...|2023-04-13T11:29:48Z|      en|        0|  1.100000023841858|                 0.5|\n",
      "| Omg 🎉🎉🎉....you...|2023-04-13T09:47:31Z|      en|        1| 0.8999999761581421|  0.8999999761581421|\n",
      "|         Lost of fun.|2023-04-13T07:36:56Z|      en|        0| 0.6000000238418579|  0.6000000238418579|\n",
      "|You’re so cute～ L...|2023-04-13T06:27:23Z|      en|        1| 0.8999999761581421|  0.8999999761581421|\n",
      "| Morning workout b...|2023-04-13T04:42:20Z|      en|        1| 0.4000000059604645|  0.4000000059604645|\n",
      "+---------------------+--------------------+--------+---------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_allcommentsinfo_unwinded_sentiment_coffee = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0415.allcommentsinfo_unwinded_sentiment').load()\n",
    "\n",
    "df_allcommentsinfo_unwinded_sentiment_emi = spark.read.format('mongo').option(\n",
    "    'spark.mongodb.input.uri', 'mongodb://172.1.0.10/youtubedataapi0415.allcommentsinfo_unwinded_sentiment2').load()\n",
    "\n",
    "df_allcommentsinfo_unwinded_sentiment_coffee.createOrReplaceTempView(\n",
    "    'df_llcommentsinfo_unwinded_sentiment_coffee')\n",
    "df_allcommentsinfo_unwinded_sentiment_emi.createOrReplaceTempView(\n",
    "    'df_llcommentsinfo_unwinded_sentiment_emi')\n",
    "\n",
    "# merge the dataframes using union\n",
    "merged_df_allcommentsinfo_sentiment = df_allcommentsinfo_unwinded_sentiment_emi.union(\n",
    "    df_allcommentsinfo_unwinded_sentiment_coffee).drop(\"_id\")\n",
    "\n",
    "# show the merged dataframe\n",
    "merged_df_allcommentsinfo_sentiment.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine dataframes of allvideocommentsinfo and allvideocomments with sentiment scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Comment: string (nullable = true)\n",
      " |-- LikeCount: integer (nullable = true)\n",
      " |-- CommentPublishedTime: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- DataRetrievedAt: string (nullable = true)\n",
      " |-- videoId: string (nullable = true)\n",
      " |-- Language: string (nullable = true)\n",
      " |-- Magnitude: double (nullable = true)\n",
      " |-- Score: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+----+-----+---+----+---------+--------------------+-----------+--------+-------------------+--------------------+\n",
      "|             Comment|LikeCount|CommentPublishedTime|year|month|day|hour|  weekday|     DataRetrievedAt|    videoId|Language|          Magnitude|               Score|\n",
      "+--------------------+---------+--------------------+----+-----+---+----+---------+--------------------+-----------+--------+-------------------+--------------------+\n",
      "|                    |        0|2017-11-05T17:54:55Z|2017|   11|  5|  17|   Sunday|2023-04-15T07:10:...|MdhbH0HNr2w|      en|                0.0|                 0.0|\n",
      "|                    |        0|2021-04-23T15:58:26Z|2021|    4| 23|  15|   Friday|2023-04-15T11:17:...|s4fwetu2Px4|      en|                0.0|                 0.0|\n",
      "|              加油⛽️|        1|2015-07-28T08:55:26Z|2015|    7| 28|   8|  Tuesday|2023-04-15T07:10:...|zT2jhR66ADo|      zh| 0.8999999761581421|  0.8999999761581421|\n",
      "|              !!😍👌|        1|2017-06-15T13:18:09Z|2017|    6| 15|  13| Thursday|2023-04-15T11:19:...|ipUAWibHL34|      en|                0.5|                 0.5|\n",
      "|\" 1,2,3,4,5,6,7\\n...|        1|2022-03-13T22:25:45Z|2022|    3| 13|  22|   Sunday|2023-04-15T11:18:...|aIMfpvdOmX8|      en|0.10000000149011612| 0.10000000149011612|\n",
      "|\"1m63 and 110lbs\"...|        2|2020-05-25T13:46:26Z|2020|    5| 25|  13|   Monday|2023-04-15T11:19:...|frpjZ5sPR-A|      de|0.20000000298023224|-0.20000000298023224|\n",
      "|\"A\" in Love, marr...|        3|2021-07-08T12:52:35Z|2021|    7|  8|  12| Thursday|2023-04-15T11:17:...|MGHgcrjwF-0|      en|0.20000000298023224|-0.10000000149011612|\n",
      "|\"Baju\" means clot...|        0|2018-12-02T12:31:02Z|2018|   12|  2|  12|   Sunday|2023-04-15T11:18:...|Df64skVTGnU|      en|                0.0|                 0.0|\n",
      "|\"But seek first t...|        1|2021-06-09T13:03:49Z|2021|    6|  9|  13|Wednesday|2023-04-15T11:17:...|r_UDA3E0dtg|      en| 0.6000000238418579| 0.30000001192092896|\n",
      "|\"Closer the movem...|        1|2022-11-20T19:47:47Z|2022|   11| 20|  19|   Sunday|2023-04-15T11:18:...|6wEGUy9cTjs|      en| 0.6000000238418579| 0.30000001192092896|\n",
      "|\"Don't worry it i...|        9|2022-01-16T17:22:38Z|2022|    1| 16|  17|   Sunday|2023-04-15T11:17:...|VUVlYOsVf30|      en|  0.699999988079071|  -0.699999988079071|\n",
      "|\"En route\" ? Do y...|        1|2018-10-24T22:28:43Z|2018|   10| 24|  22|Wednesday|2023-04-15T11:18:...|UmRfx3a7OyE|      en|                0.0|                 0.0|\n",
      "|\"Extracuricular A...|       37|2018-06-29T22:32:21Z|2018|    6| 29|  22|   Friday|2023-04-15T11:19:...|xtdozPQbSv8|      en|  7.199999809265137|                 0.5|\n",
      "|\"For to me, to li...|        1|2021-06-07T03:31:49Z|2021|    6|  7|   3|   Monday|2023-04-15T11:17:...|r_UDA3E0dtg|      en|0.20000000298023224|                 0.0|\n",
      "|\"Girlfriend cant ...|        0|2019-11-15T08:32:45Z|2019|   11| 15|   8|   Friday|2023-04-15T11:18:...|afWSGNQJI7Y|      en|0.20000000298023224|-0.20000000298023224|\n",
      "|\"Hey! I'm Chad!\" ...|        0|2019-08-30T07:59:02Z|2019|    8| 30|   7|   Friday|2023-04-15T11:19:...|AjalM-2bPN4|      en|  0.800000011920929| 0.20000000298023224|\n",
      "|\"I want to look d...|        0|2022-08-29T04:04:35Z|2022|    8| 29|   4|   Monday|2023-04-15T11:17:...|vP_nQw1KH3k|      en| 0.6000000238418579| -0.6000000238418579|\n",
      "|\"I'm a classy guy...|        8|2018-11-10T12:48:04Z|2018|   11| 10|  12| Saturday|2023-04-15T11:18:...|dwIare9U1ZU|      en| 0.8999999761581421|  0.8999999761581421|\n",
      "|\"IF IT'S PAINING,...|        1|2023-04-15T10:21:08Z|2023|    4| 15|  10| Saturday|2023-04-15T11:17:...|jXm0y-csiuE|      en|0.10000000149011612|-0.10000000149011612|\n",
      "|\"It's her fantasy...|        3|2021-01-05T15:49:18Z|2021|    1|  5|  15|  Tuesday|2023-04-15T11:18:...|wX5IgKXmhXw|      en| 0.8999999761581421|  0.4000000059604645|\n",
      "+--------------------+---------+--------------------+----+-----+---+----+---------+--------------------+-----------+--------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# combine the dataframes using join\n",
    "combined_df_allcomments = merged_df_allcommentsinfo_unwinded.join(\n",
    "    merged_df_allcommentsinfo_sentiment, [\"Comment\", \"LikeCount\", \"CommentPublishedTime\"], \"outer\")\n",
    "\n",
    "# print the schema\n",
    "combined_df_allcomments.printSchema()\n",
    "# show the combined dataframe\n",
    "combined_df_allcomments.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# select only the necessary columns from df_allvideodetails0415\n",
    "df_allvideodetails0416_selected = df_allvideodetails0418.select(\n",
    "    \"videoId\", \"ChannelTitle\")\n",
    "\n",
    "# join the two dataframes on the videoId column\n",
    "joined_comment_df = combined_df_allcomments.join(\n",
    "    df_allvideodetails0416_selected, \"videoId\", \"left_outer\").drop(df_allvideodetails0416_selected.videoId)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+---------+--------------------+----+-----+---+----+---------+--------------------+--------+-------------------+--------------------+------------+\n",
      "|    videoId|                     Comment|LikeCount|CommentPublishedTime|year|month|day|hour|  weekday|     DataRetrievedAt|Language|          Magnitude|               Score|ChannelTitle|\n",
      "+-----------+----------------------------+---------+--------------------+----+-----+---+----+---------+--------------------+--------+-------------------+--------------------+------------+\n",
      "|vAPTEmKqywk|                           \"|        0|2022-03-12T07:11:17Z|2022|    3| 12|   7| Saturday|2023-04-15T11:16:...|      en|0.20000000298023224| 0.20000000298023224|    emi wong|\n",
      "|xtdozPQbSv8|        \" oh it's good fo...|        2|2018-06-29T16:35:46Z|2018|    6| 29|  16|   Friday|2023-04-15T11:19:...|      en| 0.8999999761581421|  0.8999999761581421|    emi wong|\n",
      "|FL_fAd_K720|        \"Don't just print...|       31|2020-10-31T14:54:30Z|2020|   10| 31|  14| Saturday|2023-04-15T11:19:...|      en|                0.5|                -0.5|    emi wong|\n",
      "|Ar9e8WBnYmg|\\b一年前无做完呢个challen...|        0|2020-07-04T20:19:27Z|2020|    7|  4|  20| Saturday|2023-04-15T07:09:...|      zh| 0.8999999761581421|  0.8999999761581421|Coffee林芊妤|\n",
      "|j8JUgvDaXAQ|        !!!i just LOVE YO...|        2|2018-06-23T20:45:48Z|2018|    6| 23|  20| Saturday|2023-04-15T11:19:...|      en|  1.899999976158142|  0.8999999761581421|    emi wong|\n",
      "|SUy3-htFkys|                          \\n|        0|2015-08-29T09:04:57Z|2015|    8| 29|   9| Saturday|2023-04-15T07:10:...|      en|                0.0|                 0.0|Coffee林芊妤|\n",
      "|4MgHRtCZb40|        \"And we're awkwar...|       14|2018-09-30T13:14:46Z|2018|    9| 30|  13|   Sunday|2023-04-15T11:18:...|      en|  0.800000011920929|  -0.800000011920929|    emi wong|\n",
      "|yjlJiMNZDZA|                    !!!!!WOW|        1|2019-08-01T10:02:39Z|2019|    8|  1|  10| Thursday|2023-04-15T11:18:...|      en| 0.8999999761581421|  0.8999999761581421|    emi wong|\n",
      "|wMrLpFY38Yk|                            |        0|2023-03-26T08:17:23Z|2023|    3| 26|   8|   Sunday|2023-04-15T07:08:...|      en|                0.0|                 0.0|Coffee林芊妤|\n",
      "|IhjFPldv0Ys|        \" too bad chad is...|        0|2020-04-20T11:45:55Z|2020|    4| 20|  11|   Monday|2023-04-15T11:18:...|      en|0.20000000298023224|-0.20000000298023224|    emi wong|\n",
      "|BRAZjn0HNis|        \"15 min NO JUMPIN...|       49|2020-06-09T21:11:19Z|2020|    6|  9|  21|  Tuesday|2023-04-15T11:19:...|      en|                0.5|-0.20000000298023224|    emi wong|\n",
      "|HXjlQQ4nWnc|        \"Can you trust th...|       15|2020-09-09T17:54:35Z|2020|    9|  9|  17|Wednesday|2023-04-15T11:17:...|      en|0.20000000298023224|-0.20000000298023224|    emi wong|\n",
      "|ipUAWibHL34|        \"Don't quit on me...|       54|2017-06-17T00:28:22Z|2017|    6| 17|   0| Saturday|2023-04-15T11:19:...|      en| 1.7000000476837158|  0.4000000059604645|    emi wong|\n",
      "|oivYJgfRkzY|        \" chad's boobs ca...|        0|2020-04-17T14:01:29Z|2020|    4| 17|  14|   Friday|2023-04-15T11:18:...|      en|0.20000000298023224| 0.20000000298023224|    emi wong|\n",
      "|r_UDA3E0dtg|        \"For God hath not...|        1|2021-06-21T11:41:25Z|2021|    6| 21|  11|   Monday|2023-04-15T11:17:...|      en|0.30000001192092896|-0.10000000149011612|    emi wong|\n",
      "|r_UDA3E0dtg|        \"For I know the p...|        1|2021-06-10T09:58:29Z|2021|    6| 10|   9| Thursday|2023-04-15T11:17:...|      en|0.20000000298023224| 0.10000000149011612|    emi wong|\n",
      "|r_UDA3E0dtg|        \"For everyone who...|        1|2021-06-08T03:38:43Z|2021|    6|  8|   3|  Tuesday|2023-04-15T11:17:...|      en|  0.800000011920929|  0.4000000059604645|    emi wong|\n",
      "|tCJ6IJaM9hU|        \"Do swimming\"\\n\\n...|        0|2021-06-07T12:05:52Z|2021|    6|  7|  12|   Monday|2023-04-15T11:19:...|      en|0.20000000298023224| 0.10000000149011612|    emi wong|\n",
      "|JYuT7hT7Ka8|        \"Ahh, this one ha...|       14|2017-11-08T13:33:32Z|2017|   11|  8|  13|Wednesday|2023-04-15T11:19:...|      en|  0.800000011920929|   0.800000011920929|    emi wong|\n",
      "|FIXgzH656dk|        \"Be Honest Who el...|       18|2020-06-24T14:09:25Z|2020|    6| 24|  14|Wednesday|2023-04-15T11:17:...|      en|                1.0|                 0.0|    emi wong|\n",
      "+-----------+----------------------------+---------+--------------------+----+-----+---+----+---------+--------------------+--------+-------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_comment_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: write avro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_allcomments.write.format(\n",
    "    \"avro\").save(\"combined_df_allcomments.avro\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from spark to BigQuery\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of allcomments_sentiment to BigQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "# configure BigQuery client\n",
    "client = bigquery.Client(project=\"spheric-temple-380502\")\n",
    "\n",
    "# create BigQuery table\n",
    "table_ref = client.dataset(\"youtubedata\").table(\n",
    "    \"allcomments_sentiment_0417\")\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"videoId\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Comment\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"LikeCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"CommentPublishedTime\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"hour\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"weekday\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"DataRetrievedAt\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Language\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Magnitude\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Score\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"ChannelTitle\", \"STRING\"),\n",
    "]\n",
    "table = bigquery.Table(table_ref, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "# write the dataframe to BigQuery\n",
    "joined_comment_df.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save('spheric-temple-380502.youtubedata.allcomments_sentiment_0417')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write dataframe of allvideodetails to BigQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table spheric-temple-380502.youtubedata.allvideodetails0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "# configure BigQuery client\n",
    "client = bigquery.Client(project=\"spheric-temple-380502\")\n",
    "\n",
    "table_id = \"spheric-temple-380502.youtubedata.allvideodetails0416\"\n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"Video_Title\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"hour\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"weekday\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Caption\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"ChannelTitle\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Comments\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"DataRetrievedAt\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Definition\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Description\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Duration\", \"FLOAT\"),\n",
    "    bigquery.SchemaField(\"Likes\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"Published_date\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Tags\", \"STRING\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"VideoId\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"Views\", \"INTEGER\")\n",
    "]\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
    "\n",
    "\n",
    "df_allvideodetails0418.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save('spheric-temple-380502.youtubedata.allvideodetails0416')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table spheric-temple-380502.youtubedata.allvideodetails0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# v2\n",
    "\n",
    "from google.cloud import bigquery\n",
    "# configure BigQuery client\n",
    "client = bigquery.Client(project=\"spheric-temple-380502\")\n",
    "\n",
    "table_id = \"spheric-temple-380502.youtubedata.allvideodetails0418\"\n",
    "\n",
    "schema = [\n",
    "    {\"name\": \"year\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"month\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"day\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"hour\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"weekday\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Caption\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"ChannelTitle\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Comments\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"DataRetrievedAt\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Definition\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Description\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Duration\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Likes\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Published_date\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Tags\", \"type\": \"STRING\", \"mode\": \"REPEATED\"},\n",
    "    {\"name\": \"VideoId\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"VideoTitle\", \"type\": \"STRING\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"Views\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"TagNum\", \"type\": \"INTEGER\", \"mode\": \"REQUIRED\"},\n",
    "    {\"name\": \"likeRatio\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "    {\"name\": \"commentRatio\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\"},\n",
    "]\n",
    "\n",
    "\n",
    "table = bigquery.Table(table_id, schema=schema)\n",
    "table = client.create_table(table)\n",
    "\n",
    "print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
    "\n",
    "\n",
    "df_allvideodetails0418.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save('spheric-temple-380502.youtubedata.allvideodetails0418')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
